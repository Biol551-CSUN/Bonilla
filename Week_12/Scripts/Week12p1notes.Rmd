---
title: "Week12p1"
author: "Brandon"
date: "4/19/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro
1. Using {stringr} to clean up strings (part of tidyverse)
2. Intro to regex (regular expressions)
3. Using {tidytext} for text mining/analysis
4. Make a wordcloud

```{r}
### Load Libraries ###
library(here)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(janeaustenr)

```

## Using Stringer

```{r}
words<-"This is a string"
words
words_vector<-c("Apples", "Bananas","Oranges")
words_vector

paste("High temp", "Low pH", sep = "-")
paste0("High temp", "Low pH")

shapes <- c("Square", "Circle", "Triangle")
paste("My favorite shape is a", shapes)
two_cities <- c("best", "worst")
paste("It was the", two_cities, "of times.")

shapes # vector of shapes
str_length(shapes) # how many letters are in each word?
seq_data<-c("ATCCCGTC")

str_sub(seq_data, start = 2, end = 4) # extract the 2nd to 4th AA

str_sub(seq_data, start = 3, end = 3) <- "A" # add an A in the 3rd position
seq_data
## [1] "ATACCGTC"
str_dup(seq_data, times = c(2, 3)) # times is the number of times to duplicate each string
## [1] "ATACCGTCATACCGTC"   "ATACCGTCATACCGTCATACCGTC"

badtreatments<-c("High", " High", "High ", "Low", "Low")
badtreatments
## [1] "High"  " High" "High " "Low"   "Low"

str_trim(badtreatments) # this removes both
str_trim(badtreatments, side = "left") # this removes left
str_pad(badtreatments, 5, side = "right") # add a white space to the right side after the 5th character
str_pad(badtreatments, 5, side = "right", pad = "1") # add a 1 to the right side after the 5th character

x<-"I love R!"
str_to_upper(x)
str_to_lower(x) # make lowercase
str_to_title(x) # title case

data<-c("AAA", "TATA", "CTAG", "GCTT")
str_view(data, pattern = "A") # find all the strings with an A

str_detect(data, pattern = "A")
str_detect(data, pattern = "AT")
str_locate(data, pattern = "AT")

vals<-c("a.b", "b.c","c.d")
str_replace(vals, "\\.", " ") #string, pattern, replace

val2<-c("test 123", "test 456", "test")
str_subset(val2, "\\d")

str_count(val2, "[aeiou]")
str_count(val2, "[0-9]") # count any digit

strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
str_detect(strings, phone) # Which strings contain phone numbers?

test<-str_subset(strings, phone) # subset only the strings with phone numbers
test
```
## Think Pair Share

```{r}
test %>%
  str_replace_all("\\.", "-") %>%
  str_replace_all("[a-z]", "") %>%
  str_replace_all("[:]", "") %>%
  str_trim()

test %>%
  str_replace_all(pattern = "\\.", replacement = "-") %>% # replace periods with -
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% # remove all the things we don't want
  str_trim() # trim the white space
```

## More Stringer
```{r}
head(austen_books()) # explore tidytext
tail(austen_books())

original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>%
  mutate(line = row_number(), # find every line
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", # count the chapters 
  ignore_case = TRUE)))) %>% #ignore lower or uppercase
  ungroup() # ungroup it so we have a dataframe again
# don't try to view the entire thing
head(original_books)

tidy_books <- original_books %>%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column
head(tidy_books) # Don't view the entire thing!

head(get_stopwords()) #see an example of all the stopwords
cleaned_books <- tidy_books %>%
  anti_join(get_stopwords()) # dataframe without the stopwords
head(cleaned_books)
cleaned_books %>%
  count(word, sort = TRUE)

sent_word_counts <- tidy_books %>%
  inner_join(get_sentiments()) %>% # only keep + or - words
  count(word, sentiment, sort = TRUE) # count them

sent_word_counts %>%
  filter(n > 150) %>% 
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% 
  mutate(word = reorder(word, n)) %>% # largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")

words<-cleaned_books %>%
  count(word) %>% # count all the words
  arrange(desc(n))%>% # sort the words
  slice(1:100) #take the top 100
wordcloud2(words, shape = 'triangle', size=0.3)
```

